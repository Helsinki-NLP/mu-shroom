{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing dependencies. You might need to tweak the CMAKE_ARGS for the `llama-cpp-python` pip package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKL68Itp9Bm-",
    "outputId": "dd33c010-aa3e-4f6a-c763-e30047591c5e"
   },
   "outputs": [],
   "source": [
    "# GPU llama-cpp-python; Starting from version llama-cpp-python==0.1.79, it supports GGUF\n",
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=off \" pip install 'llama-cpp-python>=0.1.79' --force-reinstall --upgrade --no-cache-dir\n",
    "# For download the models\n",
    "# !pip install huggingface_hub\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading an instruction-finetuned Mistral model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106,
     "referenced_widgets": [
      "2ae89d1a8a074a249b750d138587e44d",
      "eb30e73c1e824fa8942f0c58104d696f",
      "df0a135d8a5b43d5ab94bef15b2db5aa",
      "a5e99c0d3739407799fde2f29a301d05",
      "fa5555299e2e47ae9d2cc7a7e58415f4",
      "c96a1b051a7b4fbfbd873be07cf44cf0",
      "fa37a3f2205749468f31309b6061ffef",
      "a0ceffacff7f492d87084da291061006",
      "af87959da48a436e842f58ac691717df",
      "e35a5293e19748679095d1222f1a31e5",
      "2abefc6082af406ab1c955a880a2b419"
     ]
    },
    "id": "uDMqQmBfAhYO",
    "outputId": "eacd2078-6e5a-4451-84b4-69c6789cb4d1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/mickus/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 8000\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
    "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\"\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "\n",
    "# This config has been tested on an RTX 3080 (VRAM of 16GB).\n",
    "# you might need to tweak with respect to your hardware.\n",
    "from llama_cpp import Llama\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=4, # CPU cores\n",
    "    n_batch=8000, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    # n_gpu_layers=32, # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=8192, # Context window\n",
    "    logits_all=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "import json \n",
    "import csv\n",
    "\n",
    "with open('english-with-questions-valid+test.tsv', 'r') as istr:\n",
    "    reader = csv.reader(istr, delimiter='\\t')\n",
    "    header = next(reader)\n",
    "    records = [dict(zip(header, row)) for row in reader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d5ccfacf6645668e5364453d65e647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configs:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba34ba396934285921a93a5366c7523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "items:   0%|          | 0/207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      32.46 ms /    85 runs   (    0.38 ms per token,  2618.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3260.59 ms /    19 tokens (  171.61 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:        eval time =   24825.87 ms /    84 runs   (  295.55 ms per token,     3.38 tokens per second)\n",
      "llama_print_timings:       total time =   28180.98 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      12.53 ms /    46 runs   (    0.27 ms per token,  3672.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2704.75 ms /    18 tokens (  150.26 ms per token,     6.65 tokens per second)\n",
      "llama_print_timings:        eval time =   12957.59 ms /    45 runs   (  287.95 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   15705.08 ms /    63 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      40.99 ms /    92 runs   (    0.45 ms per token,  2244.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1725.63 ms /    17 tokens (  101.51 ms per token,     9.85 tokens per second)\n",
      "llama_print_timings:        eval time =   28122.46 ms /    91 runs   (  309.04 ms per token,     3.24 tokens per second)\n",
      "llama_print_timings:       total time =   29958.98 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      44.10 ms /   109 runs   (    0.40 ms per token,  2471.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1505.49 ms /    15 tokens (  100.37 ms per token,     9.96 tokens per second)\n",
      "llama_print_timings:        eval time =   33986.32 ms /   108 runs   (  314.69 ms per token,     3.18 tokens per second)\n",
      "llama_print_timings:       total time =   35635.14 ms /   123 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      31.73 ms /    80 runs   (    0.40 ms per token,  2520.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3269.89 ms /    20 tokens (  163.49 ms per token,     6.12 tokens per second)\n",
      "llama_print_timings:        eval time =   24183.88 ms /    79 runs   (  306.13 ms per token,     3.27 tokens per second)\n",
      "llama_print_timings:       total time =   27539.28 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =     131.27 ms /   299 runs   (    0.44 ms per token,  2277.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3306.83 ms /    22 tokens (  150.31 ms per token,     6.65 tokens per second)\n",
      "llama_print_timings:        eval time =   92065.16 ms /   298 runs   (  308.94 ms per token,     3.24 tokens per second)\n",
      "llama_print_timings:       total time =   95789.11 ms /   320 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      29.13 ms /    84 runs   (    0.35 ms per token,  2883.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2039.94 ms /    20 tokens (  102.00 ms per token,     9.80 tokens per second)\n",
      "llama_print_timings:        eval time =   24006.60 ms /    83 runs   (  289.24 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =   26127.59 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      67.65 ms /   148 runs   (    0.46 ms per token,  2187.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2393.71 ms /    18 tokens (  132.98 ms per token,     7.52 tokens per second)\n",
      "llama_print_timings:        eval time =   46264.75 ms /   147 runs   (  314.73 ms per token,     3.18 tokens per second)\n",
      "llama_print_timings:       total time =   48845.75 ms /   165 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      29.74 ms /    83 runs   (    0.36 ms per token,  2790.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1887.42 ms /    17 tokens (  111.02 ms per token,     9.01 tokens per second)\n",
      "llama_print_timings:        eval time =   24275.93 ms /    82 runs   (  296.05 ms per token,     3.38 tokens per second)\n",
      "llama_print_timings:       total time =   26249.03 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      44.70 ms /   111 runs   (    0.40 ms per token,  2483.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1269.51 ms /    13 tokens (   97.65 ms per token,    10.24 tokens per second)\n",
      "llama_print_timings:        eval time =   33467.35 ms /   110 runs   (  304.25 ms per token,     3.29 tokens per second)\n",
      "llama_print_timings:       total time =   34862.27 ms /   123 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      72.09 ms /   171 runs   (    0.42 ms per token,  2372.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1950.59 ms /    15 tokens (  130.04 ms per token,     7.69 tokens per second)\n",
      "llama_print_timings:        eval time =   52704.39 ms /   170 runs   (  310.03 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =   54915.33 ms /   185 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      20.38 ms /    61 runs   (    0.33 ms per token,  2992.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1617.11 ms /    16 tokens (  101.07 ms per token,     9.89 tokens per second)\n",
      "llama_print_timings:        eval time =   17418.10 ms /    60 runs   (  290.30 ms per token,     3.44 tokens per second)\n",
      "llama_print_timings:       total time =   19093.05 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      55.78 ms /   129 runs   (    0.43 ms per token,  2312.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1182.74 ms /    11 tokens (  107.52 ms per token,     9.30 tokens per second)\n",
      "llama_print_timings:        eval time =   39710.42 ms /   128 runs   (  310.24 ms per token,     3.22 tokens per second)\n",
      "llama_print_timings:       total time =   41049.33 ms /   139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      21.41 ms /    62 runs   (    0.35 ms per token,  2896.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1791.04 ms /    16 tokens (  111.94 ms per token,     8.93 tokens per second)\n",
      "llama_print_timings:        eval time =   18163.32 ms /    61 runs   (  297.76 ms per token,     3.36 tokens per second)\n",
      "llama_print_timings:       total time =   20015.41 ms /    77 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      23.09 ms /    60 runs   (    0.38 ms per token,  2597.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2297.37 ms /    16 tokens (  143.59 ms per token,     6.96 tokens per second)\n",
      "llama_print_timings:        eval time =   17951.53 ms /    59 runs   (  304.26 ms per token,     3.29 tokens per second)\n",
      "llama_print_timings:       total time =   20312.47 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      44.69 ms /   104 runs   (    0.43 ms per token,  2326.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1243.99 ms /    10 tokens (  124.40 ms per token,     8.04 tokens per second)\n",
      "llama_print_timings:        eval time =   32236.48 ms /   103 runs   (  312.98 ms per token,     3.20 tokens per second)\n",
      "llama_print_timings:       total time =   33604.36 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      42.14 ms /   113 runs   (    0.37 ms per token,  2681.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4449.53 ms /    21 tokens (  211.88 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:        eval time =   34749.99 ms /   112 runs   (  310.27 ms per token,     3.22 tokens per second)\n",
      "llama_print_timings:       total time =   39343.79 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      24.86 ms /    60 runs   (    0.41 ms per token,  2413.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2263.04 ms /    15 tokens (  150.87 ms per token,     6.63 tokens per second)\n",
      "llama_print_timings:        eval time =   17221.83 ms /    59 runs   (  291.90 ms per token,     3.43 tokens per second)\n",
      "llama_print_timings:       total time =   19552.32 ms /    74 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      51.26 ms /   123 runs   (    0.42 ms per token,  2399.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1303.85 ms /    13 tokens (  100.30 ms per token,     9.97 tokens per second)\n",
      "llama_print_timings:        eval time =   36748.06 ms /   122 runs   (  301.21 ms per token,     3.32 tokens per second)\n",
      "llama_print_timings:       total time =   38196.83 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      21.91 ms /    71 runs   (    0.31 ms per token,  3241.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1753.29 ms /    17 tokens (  103.13 ms per token,     9.70 tokens per second)\n",
      "llama_print_timings:        eval time =   20705.14 ms /    70 runs   (  295.79 ms per token,     3.38 tokens per second)\n",
      "llama_print_timings:       total time =   22533.62 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      72.13 ms /   170 runs   (    0.42 ms per token,  2356.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2644.91 ms /    19 tokens (  139.21 ms per token,     7.18 tokens per second)\n",
      "llama_print_timings:        eval time =   53530.54 ms /   169 runs   (  316.75 ms per token,     3.16 tokens per second)\n",
      "llama_print_timings:       total time =   56384.07 ms /   188 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      17.41 ms /    64 runs   (    0.27 ms per token,  3676.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1645.15 ms /    13 tokens (  126.55 ms per token,     7.90 tokens per second)\n",
      "llama_print_timings:        eval time =   18757.32 ms /    63 runs   (  297.74 ms per token,     3.36 tokens per second)\n",
      "llama_print_timings:       total time =   20465.07 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      20.08 ms /    59 runs   (    0.34 ms per token,  2938.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2969.08 ms /    19 tokens (  156.27 ms per token,     6.40 tokens per second)\n",
      "llama_print_timings:        eval time =   18279.21 ms /    58 runs   (  315.16 ms per token,     3.17 tokens per second)\n",
      "llama_print_timings:       total time =   21315.97 ms /    77 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      42.63 ms /   101 runs   (    0.42 ms per token,  2369.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1694.80 ms /    16 tokens (  105.92 ms per token,     9.44 tokens per second)\n",
      "llama_print_timings:        eval time =   31040.30 ms /   100 runs   (  310.40 ms per token,     3.22 tokens per second)\n",
      "llama_print_timings:       total time =   32853.05 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      19.26 ms /    60 runs   (    0.32 ms per token,  3115.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1435.37 ms /    15 tokens (   95.69 ms per token,    10.45 tokens per second)\n",
      "llama_print_timings:        eval time =   15710.58 ms /    59 runs   (  266.28 ms per token,     3.76 tokens per second)\n",
      "llama_print_timings:       total time =   17197.96 ms /    74 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      67.35 ms /   161 runs   (    0.42 ms per token,  2390.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2035.55 ms /    22 tokens (   92.53 ms per token,    10.81 tokens per second)\n",
      "llama_print_timings:        eval time =   46412.75 ms /   160 runs   (  290.08 ms per token,     3.45 tokens per second)\n",
      "llama_print_timings:       total time =   48681.62 ms /   182 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      17.34 ms /    59 runs   (    0.29 ms per token,  3401.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1859.32 ms /    14 tokens (  132.81 ms per token,     7.53 tokens per second)\n",
      "llama_print_timings:        eval time =   16810.23 ms /    58 runs   (  289.83 ms per token,     3.45 tokens per second)\n",
      "llama_print_timings:       total time =   18724.64 ms /    72 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      27.36 ms /    82 runs   (    0.33 ms per token,  2996.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2207.70 ms /    14 tokens (  157.69 ms per token,     6.34 tokens per second)\n",
      "llama_print_timings:        eval time =   23571.42 ms /    81 runs   (  291.01 ms per token,     3.44 tokens per second)\n",
      "llama_print_timings:       total time =   25858.26 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      29.12 ms /    86 runs   (    0.34 ms per token,  2953.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1541.10 ms /    14 tokens (  110.08 ms per token,     9.08 tokens per second)\n",
      "llama_print_timings:        eval time =   24473.89 ms /    85 runs   (  287.93 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   26099.29 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =     122.67 ms /   294 runs   (    0.42 ms per token,  2396.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2163.84 ms /    13 tokens (  166.45 ms per token,     6.01 tokens per second)\n",
      "llama_print_timings:        eval time =   93102.61 ms /   293 runs   (  317.76 ms per token,     3.15 tokens per second)\n",
      "llama_print_timings:       total time =   95748.26 ms /   306 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      40.72 ms /   106 runs   (    0.38 ms per token,  2603.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2680.57 ms /    17 tokens (  157.68 ms per token,     6.34 tokens per second)\n",
      "llama_print_timings:        eval time =   31297.52 ms /   105 runs   (  298.07 ms per token,     3.35 tokens per second)\n",
      "llama_print_timings:       total time =   34094.52 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =     184.08 ms /   406 runs   (    0.45 ms per token,  2205.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1320.79 ms /    14 tokens (   94.34 ms per token,    10.60 tokens per second)\n",
      "llama_print_timings:        eval time =  128449.93 ms /   405 runs   (  317.16 ms per token,     3.15 tokens per second)\n",
      "llama_print_timings:       total time =  130487.41 ms /   419 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      15.88 ms /    55 runs   (    0.29 ms per token,  3462.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1992.56 ms /    15 tokens (  132.84 ms per token,     7.53 tokens per second)\n",
      "llama_print_timings:        eval time =   15184.17 ms /    54 runs   (  281.19 ms per token,     3.56 tokens per second)\n",
      "llama_print_timings:       total time =   17228.41 ms /    69 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      31.73 ms /    86 runs   (    0.37 ms per token,  2710.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1359.14 ms /    13 tokens (  104.55 ms per token,     9.56 tokens per second)\n",
      "llama_print_timings:        eval time =   24646.62 ms /    85 runs   (  289.96 ms per token,     3.45 tokens per second)\n",
      "llama_print_timings:       total time =   26095.17 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      24.27 ms /    73 runs   (    0.33 ms per token,  3008.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1950.89 ms /    21 tokens (   92.90 ms per token,    10.76 tokens per second)\n",
      "llama_print_timings:        eval time =   20268.36 ms /    72 runs   (  281.51 ms per token,     3.55 tokens per second)\n",
      "llama_print_timings:       total time =   22299.96 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      57.24 ms /   140 runs   (    0.41 ms per token,  2446.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1522.16 ms /    16 tokens (   95.13 ms per token,    10.51 tokens per second)\n",
      "llama_print_timings:        eval time =   43001.40 ms /   139 runs   (  309.36 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =   44721.77 ms /   155 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      63.05 ms /   151 runs   (    0.42 ms per token,  2394.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5202.38 ms /    23 tokens (  226.19 ms per token,     4.42 tokens per second)\n",
      "llama_print_timings:        eval time =   43768.94 ms /   150 runs   (  291.79 ms per token,     3.43 tokens per second)\n",
      "llama_print_timings:       total time =   49146.45 ms /   173 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =     101.01 ms /   240 runs   (    0.42 ms per token,  2376.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1548.72 ms /    16 tokens (   96.79 ms per token,    10.33 tokens per second)\n",
      "llama_print_timings:        eval time =   72076.74 ms /   239 runs   (  301.58 ms per token,     3.32 tokens per second)\n",
      "llama_print_timings:       total time =   73998.38 ms /   255 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      31.07 ms /   102 runs   (    0.30 ms per token,  3283.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2100.65 ms /    15 tokens (  140.04 ms per token,     7.14 tokens per second)\n",
      "llama_print_timings:        eval time =   29140.24 ms /   101 runs   (  288.52 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   31347.81 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      37.55 ms /   102 runs   (    0.37 ms per token,  2716.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1425.48 ms /    14 tokens (  101.82 ms per token,     9.82 tokens per second)\n",
      "llama_print_timings:        eval time =   28743.34 ms /   101 runs   (  284.59 ms per token,     3.51 tokens per second)\n",
      "llama_print_timings:       total time =   30270.75 ms /   115 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      36.16 ms /    96 runs   (    0.38 ms per token,  2654.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1237.85 ms /    13 tokens (   95.22 ms per token,    10.50 tokens per second)\n",
      "llama_print_timings:        eval time =   26533.51 ms /    95 runs   (  279.30 ms per token,     3.58 tokens per second)\n",
      "llama_print_timings:       total time =   27868.38 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      11.61 ms /    38 runs   (    0.31 ms per token,  3273.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1709.14 ms /    16 tokens (  106.82 ms per token,     9.36 tokens per second)\n",
      "llama_print_timings:        eval time =   10044.42 ms /    37 runs   (  271.47 ms per token,     3.68 tokens per second)\n",
      "llama_print_timings:       total time =   11786.39 ms /    53 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      95.67 ms /   220 runs   (    0.43 ms per token,  2299.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2298.94 ms /    21 tokens (  109.47 ms per token,     9.13 tokens per second)\n",
      "llama_print_timings:        eval time =   64849.79 ms /   219 runs   (  296.12 ms per token,     3.38 tokens per second)\n",
      "llama_print_timings:       total time =   67431.89 ms /   240 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      41.61 ms /   114 runs   (    0.37 ms per token,  2739.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1322.05 ms /    12 tokens (  110.17 ms per token,     9.08 tokens per second)\n",
      "llama_print_timings:        eval time =   31994.88 ms /   113 runs   (  283.14 ms per token,     3.53 tokens per second)\n",
      "llama_print_timings:       total time =   33432.29 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      41.07 ms /   104 runs   (    0.39 ms per token,  2532.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2335.72 ms /    25 tokens (   93.43 ms per token,    10.70 tokens per second)\n",
      "llama_print_timings:        eval time =   29664.99 ms /   103 runs   (  288.01 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   32123.73 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      42.74 ms /   123 runs   (    0.35 ms per token,  2877.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1850.91 ms /    19 tokens (   97.42 ms per token,    10.27 tokens per second)\n",
      "llama_print_timings:        eval time =   34979.47 ms /   122 runs   (  286.72 ms per token,     3.49 tokens per second)\n",
      "llama_print_timings:       total time =   36980.97 ms /   141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      32.13 ms /    92 runs   (    0.35 ms per token,  2863.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2895.30 ms /    22 tokens (  131.60 ms per token,     7.60 tokens per second)\n",
      "llama_print_timings:        eval time =   26318.90 ms /    91 runs   (  289.22 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =   29307.51 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      29.80 ms /    89 runs   (    0.33 ms per token,  2986.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1564.35 ms /    12 tokens (  130.36 ms per token,     7.67 tokens per second)\n",
      "llama_print_timings:        eval time =   25380.21 ms /    88 runs   (  288.41 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:       total time =   27046.53 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =     206.79 ms /   463 runs   (    0.45 ms per token,  2239.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2110.84 ms /    21 tokens (  100.52 ms per token,     9.95 tokens per second)\n",
      "llama_print_timings:        eval time =  143010.77 ms /   462 runs   (  309.55 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =  145810.47 ms /   483 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      36.13 ms /   126 runs   (    0.29 ms per token,  3487.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1990.68 ms /    14 tokens (  142.19 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:        eval time =   34790.86 ms /   125 runs   (  278.33 ms per token,     3.59 tokens per second)\n",
      "llama_print_timings:       total time =   36914.40 ms /   139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      90.64 ms /   214 runs   (    0.42 ms per token,  2360.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1318.71 ms /    14 tokens (   94.19 ms per token,    10.62 tokens per second)\n",
      "llama_print_timings:        eval time =   63140.09 ms /   213 runs   (  296.43 ms per token,     3.37 tokens per second)\n",
      "llama_print_timings:       total time =   64718.95 ms /   227 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3260.82 ms\n",
      "llama_print_timings:      sample time =      30.32 ms /   102 runs   (    0.30 ms per token,  3363.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1733.20 ms /    17 tokens (  101.95 ms per token,     9.81 tokens per second)\n",
      "llama_print_timings:        eval time =   28444.33 ms /   101 runs   (  281.63 ms per token,     3.55 tokens per second)\n",
      "llama_print_timings:       total time =   30281.58 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "configs = [\n",
    "    ('k50_p0.90_t0.1', dict(top_k=50, top_p=0.90, temperature=0.1)),\n",
    "    ('k50_p0.95_t0.1', dict(top_k=50, top_p=0.95, temperature=0.1)),\n",
    "    ('k50_p0.90_t0.2', dict(top_k=50, top_p=0.90, temperature=0.2)),\n",
    "    ('k50_p0.95_t0.2', dict(top_k=50, top_p=0.95, temperature=0.2)),\n",
    "    ('k50_p0.90_t0.3', dict(top_k=50, top_p=0.90, temperature=0.3)),\n",
    "    ('k50_p0.95_t0.3', dict(top_k=50, top_p=0.95, temperature=0.3)),\n",
    "    ('k75_p0.90_t0.1', dict(top_k=75, top_p=0.90, temperature=0.1)),\n",
    "    ('k75_p0.95_t0.1', dict(top_k=75, top_p=0.95, temperature=0.1)),\n",
    "    ('k75_p0.90_t0.2', dict(top_k=75, top_p=0.90, temperature=0.2)),\n",
    "    ('k75_p0.95_t0.2', dict(top_k=75, top_p=0.95, temperature=0.2)),\n",
    "    ('k75_p0.90_t0.3', dict(top_k=75, top_p=0.90, temperature=0.3)),\n",
    "    ('k75_p0.95_t0.3', dict(top_k=75, top_p=0.95, temperature=0.3)),\n",
    "]\n",
    "\n",
    "random.shuffle(configs)\n",
    "\n",
    "for shorthand, config in tqdm.tqdm(configs, desc='configs'):\n",
    "    with open(f'mistral-answers-with-logprobs.{shorthand}.jsonl', 'w') as ostr_logprobs, \\\n",
    "    open(f'mistral-answers.{shorthand}.jsonl', 'w') as ostr:\n",
    "        for record in tqdm.tqdm(records, desc='items'):\n",
    "            message = record['question']\n",
    "            prompt = f\"[INST] {message} [/INST]\"\n",
    "            if 'alt_question' in record:\n",
    "                del record['alt_question']\n",
    "            response = lcpp_llm(\n",
    "                prompt=prompt,\n",
    "                logprobs=32_000,\n",
    "                max_tokens=512,\n",
    "                **config\n",
    "            )\n",
    "            print(\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        **record, \n",
    "                        'model_output': response['choices'][0]['text'],\n",
    "                        'tokens': response['choices'][0]['logprobs']['tokens'],\n",
    "                        'logprobs': [\n",
    "                            {k: float(v) for k,v in d.items()} \n",
    "                            for d in response['choices'][0]['logprobs']['top_logprobs']\n",
    "                        ],\n",
    "                        'lang': 'EN',\n",
    "                    }\n",
    "                ), \n",
    "                file=ostr_logprobs,\n",
    "            )\n",
    "            \n",
    "            print(\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        **record, \n",
    "                        'model_output': response['choices'][0]['text'],\n",
    "                        'tokens': response['choices'][0]['logprobs']['tokens'],\n",
    "                        'lang': 'EN',\n",
    "                    }\n",
    "                ), \n",
    "                file=ostr,\n",
    "                flush=True,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04b2b191f387469facbc7e0f63edd957": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c18583fabf94cf88d89e9d0ad83cd46",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_16ceb8ceabea4adeb2ed5d3c62a52e87",
      "value": 1
     }
    },
    "07bb3c8d23084467b680d0f8be879bcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08db236b9ee74ccb9ac456bf09e298e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ea36c0ff6cd4559bf733fb73ff82693": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fca89659d3684477bb46613bbb96383d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_265b13864e334d2d8875d1de157c428a",
      "value": 1
     }
    },
    "16ceb8ceabea4adeb2ed5d3c62a52e87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1c18583fabf94cf88d89e9d0ad83cd46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "265b13864e334d2d8875d1de157c428a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2abefc6082af406ab1c955a880a2b419": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ae89d1a8a074a249b750d138587e44d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eb30e73c1e824fa8942f0c58104d696f",
       "IPY_MODEL_df0a135d8a5b43d5ab94bef15b2db5aa",
       "IPY_MODEL_a5e99c0d3739407799fde2f29a301d05"
      ],
      "layout": "IPY_MODEL_fa5555299e2e47ae9d2cc7a7e58415f4"
     }
    },
    "2b25549d8eac4efd99bf1beb4fb26b0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e1566a3d2f64b5fbbaf7cc51b9c9902": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48be64dd9497468f83d73bd119591271": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cebd82bbc195424a908c9527ee1a21d3",
      "placeholder": "​",
      "style": "IPY_MODEL_8665cfefbc984fc4873e73cd96d6c018",
      "value": "Downloading data files: 100%"
     }
    },
    "4f891d2316604dd08cd5ffd22c8854d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c70248a7e6e45199ed626fa68037174",
      "placeholder": "​",
      "style": "IPY_MODEL_07bb3c8d23084467b680d0f8be879bcd",
      "value": "Generating val split: "
     }
    },
    "4facca9ecbd74aa5b4dc474634686064": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c70248a7e6e45199ed626fa68037174": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c4a2676871e492897d305d6d9a6fac9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "823cdbf0fa2c43559d01de4664258a86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8665cfefbc984fc4873e73cd96d6c018": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86da540e05824f2c95b5c8bea9b4581d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d1f94d67f08449439e3191bcdf87c6bf",
       "IPY_MODEL_cb886b4dac084c0290e1fd1c229b092e",
       "IPY_MODEL_8b8fd80c79c54e479b15f798bc545b96"
      ],
      "layout": "IPY_MODEL_3e1566a3d2f64b5fbbaf7cc51b9c9902"
     }
    },
    "8b8fd80c79c54e479b15f798bc545b96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08db236b9ee74ccb9ac456bf09e298e1",
      "placeholder": "​",
      "style": "IPY_MODEL_977e8b1928ec42a285804dcc8fc13cb5",
      "value": " 1/1 [00:00&lt;00:00,  1.86it/s]"
     }
    },
    "977e8b1928ec42a285804dcc8fc13cb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f4e1bc76cfb4643877686a6f0271b52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ceffacff7f492d87084da291061006": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0f2fe09ab0a4a21acda513f96bb7faf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4f891d2316604dd08cd5ffd22c8854d9",
       "IPY_MODEL_0ea36c0ff6cd4559bf733fb73ff82693",
       "IPY_MODEL_de38e0a8f5a24cbdbf755db3cfd399ec"
      ],
      "layout": "IPY_MODEL_9f4e1bc76cfb4643877686a6f0271b52"
     }
    },
    "a5e99c0d3739407799fde2f29a301d05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e35a5293e19748679095d1222f1a31e5",
      "placeholder": "​",
      "style": "IPY_MODEL_2abefc6082af406ab1c955a880a2b419",
      "value": " 5.94G/5.94G [00:45&lt;00:00, 157MB/s]"
     }
    },
    "ac217ebd99d94729ac89ed81fc0a0ab5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeaed97ed3f441e9aa2ce24c87e02d87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af87959da48a436e842f58ac691717df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aff193ecfc2e4d5a8b3ddd4f63604e63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48be64dd9497468f83d73bd119591271",
       "IPY_MODEL_04b2b191f387469facbc7e0f63edd957",
       "IPY_MODEL_e225b3758fa24df3a0d6f1a039d3220a"
      ],
      "layout": "IPY_MODEL_aeaed97ed3f441e9aa2ce24c87e02d87"
     }
    },
    "c96a1b051a7b4fbfbd873be07cf44cf0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb886b4dac084c0290e1fd1c229b092e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4facca9ecbd74aa5b4dc474634686064",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f52b2088b6724e6dad9ee18ba364c009",
      "value": 1
     }
    },
    "cebd82bbc195424a908c9527ee1a21d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1f94d67f08449439e3191bcdf87c6bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac217ebd99d94729ac89ed81fc0a0ab5",
      "placeholder": "​",
      "style": "IPY_MODEL_2b25549d8eac4efd99bf1beb4fb26b0c",
      "value": "Extracting data files: 100%"
     }
    },
    "de38e0a8f5a24cbdbf755db3cfd399ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_823cdbf0fa2c43559d01de4664258a86",
      "placeholder": "​",
      "style": "IPY_MODEL_e5ae38c7214c4f05974de99e5d5c3485",
      "value": " 499/0 [00:00&lt;00:00, 2393.49 examples/s]"
     }
    },
    "df0a135d8a5b43d5ab94bef15b2db5aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0ceffacff7f492d87084da291061006",
      "max": 5942065440,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af87959da48a436e842f58ac691717df",
      "value": 5942065440
     }
    },
    "e225b3758fa24df3a0d6f1a039d3220a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c4a2676871e492897d305d6d9a6fac9",
      "placeholder": "​",
      "style": "IPY_MODEL_f432e32a03704652a5bcd21c7ce36abd",
      "value": " 1/1 [00:00&lt;00:00, 29.62it/s]"
     }
    },
    "e35a5293e19748679095d1222f1a31e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5ae38c7214c4f05974de99e5d5c3485": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb30e73c1e824fa8942f0c58104d696f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c96a1b051a7b4fbfbd873be07cf44cf0",
      "placeholder": "​",
      "style": "IPY_MODEL_fa37a3f2205749468f31309b6061ffef",
      "value": "mistral-7b-instruct-v0.2.Q6_K.gguf: 100%"
     }
    },
    "f432e32a03704652a5bcd21c7ce36abd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f52b2088b6724e6dad9ee18ba364c009": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fa37a3f2205749468f31309b6061ffef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa5555299e2e47ae9d2cc7a7e58415f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fca89659d3684477bb46613bbb96383d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
