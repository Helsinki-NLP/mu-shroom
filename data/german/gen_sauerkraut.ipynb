{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing dependencies. You might need to tweak the CMAKE_ARGS for the `llama-cpp-python` pip package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKL68Itp9Bm-",
    "outputId": "dd33c010-aa3e-4f6a-c763-e30047591c5e"
   },
   "outputs": [],
   "source": [
    "# GPU llama-cpp-python; Starting from version llama-cpp-python==0.1.79, it supports GGUF\n",
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on \" pip install 'llama-cpp-python>=0.1.79' --force-reinstall --upgrade --no-cache-dir\n",
    "# For download the models\n",
    "# !pip install huggingface_hub\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading an instruction-finetuned Mistral model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106,
     "referenced_widgets": [
      "2ae89d1a8a074a249b750d138587e44d",
      "eb30e73c1e824fa8942f0c58104d696f",
      "df0a135d8a5b43d5ab94bef15b2db5aa",
      "a5e99c0d3739407799fde2f29a301d05",
      "fa5555299e2e47ae9d2cc7a7e58415f4",
      "c96a1b051a7b4fbfbd873be07cf44cf0",
      "fa37a3f2205749468f31309b6061ffef",
      "a0ceffacff7f492d87084da291061006",
      "af87959da48a436e842f58ac691717df",
      "e35a5293e19748679095d1222f1a31e5",
      "2abefc6082af406ab1c955a880a2b419"
     ]
    },
    "id": "uDMqQmBfAhYO",
    "outputId": "eacd2078-6e5a-4451-84b4-69c6789cb4d1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /users/mickusti/.cache/huggingface/hub/models--TheBloke--SauerkrautLM-7B-v1-GGUF/snapshots/5fdfb9e54142127e22f6873c68c3c2d2acee29e0/sauerkrautlm-7b-v1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3820.94 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 8192\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '0', 'llama.block_count': '32', 'llama.rope.dimension_count': '128', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.eos_token_id': '2', 'llama.feed_forward_length': '11008', 'llama.context_length': '4096', 'general.architecture': 'llama', 'llama.embedding_length': '4096', 'general.name': 'LLaMA v2', 'llama.attention.head_count': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '1', 'llama.rope.freq_base': '10000.000000', 'general.file_type': '15', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name_or_path = \"TheBloke/SauerkrautLM-7B-v1-GGUF\"\n",
    "model_basename = \"sauerkrautlm-7b-v1.Q4_K_M.gguf\"\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "\n",
    "# This config has been tested on an RTX 3080 (VRAM of 16GB).\n",
    "# you might need to tweak with respect to your hardware.\n",
    "from llama_cpp import Llama\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=4, # CPU cores\n",
    "    n_batch=8192, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=400, # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=8192, # Context window\n",
    "    logits_all=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "import json \n",
    "import csv\n",
    "\n",
    "with open('questions-de.tsv', 'r') as istr:\n",
    "    reader = csv.reader(istr, delimiter='\\t')\n",
    "    header = next(reader)\n",
    "    records = [dict(zip(header, row)) for row in reader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8789034c75b1444fb59091100ae3a95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configs:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7f7767ad9c4f54a19343f98c3b37df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "items:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      11.93 ms /    28 runs   (    0.43 ms per token,  2347.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     198.98 ms /    25 tokens (    7.96 ms per token,   125.64 tokens per second)\n",
      "llama_print_timings:        eval time =     265.59 ms /    27 runs   (    9.84 ms per token,   101.66 tokens per second)\n",
      "llama_print_timings:       total time =     696.12 ms /    52 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     1 runs   (    0.44 ms per token,  2272.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =      35.23 ms /    25 tokens (    1.41 ms per token,   709.64 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     160.15 ms /    26 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       4.77 ms /    11 runs   (    0.43 ms per token,  2307.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =      27.98 ms /    22 tokens (    1.27 ms per token,   786.30 tokens per second)\n",
      "llama_print_timings:        eval time =      94.55 ms /    10 runs   (    9.45 ms per token,   105.77 tokens per second)\n",
      "llama_print_timings:       total time =     131.93 ms /    32 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      15.71 ms /    35 runs   (    0.45 ms per token,  2227.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =      27.76 ms /    20 tokens (    1.39 ms per token,   720.51 tokens per second)\n",
      "llama_print_timings:        eval time =     316.75 ms /    34 runs   (    9.32 ms per token,   107.34 tokens per second)\n",
      "llama_print_timings:       total time =     372.55 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    14 runs   (    0.43 ms per token,  2323.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     413.86 ms /    24 tokens (   17.24 ms per token,    57.99 tokens per second)\n",
      "llama_print_timings:        eval time =     121.47 ms /    13 runs   (    9.34 ms per token,   107.02 tokens per second)\n",
      "llama_print_timings:       total time =    1039.29 ms /    37 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      27.00 ms /    64 runs   (    0.42 ms per token,  2370.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =      27.80 ms /    22 tokens (    1.26 ms per token,   791.51 tokens per second)\n",
      "llama_print_timings:        eval time =     591.27 ms /    63 runs   (    9.39 ms per token,   106.55 tokens per second)\n",
      "llama_print_timings:       total time =     821.37 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      10.46 ms /    25 runs   (    0.42 ms per token,  2389.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =      76.70 ms /    18 tokens (    4.26 ms per token,   234.68 tokens per second)\n",
      "llama_print_timings:        eval time =     226.69 ms /    24 runs   (    9.45 ms per token,   105.87 tokens per second)\n",
      "llama_print_timings:       total time =     780.35 ms /    42 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       9.93 ms /    25 runs   (    0.40 ms per token,  2516.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =      26.18 ms /    16 tokens (    1.64 ms per token,   611.15 tokens per second)\n",
      "llama_print_timings:        eval time =     237.26 ms /    24 runs   (    9.89 ms per token,   101.15 tokens per second)\n",
      "llama_print_timings:       total time =     404.30 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       7.45 ms /    18 runs   (    0.41 ms per token,  2417.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =      27.71 ms /    18 tokens (    1.54 ms per token,   649.49 tokens per second)\n",
      "llama_print_timings:        eval time =     159.45 ms /    17 runs   (    9.38 ms per token,   106.62 tokens per second)\n",
      "llama_print_timings:       total time =     371.06 ms /    35 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       4.56 ms /    10 runs   (    0.46 ms per token,  2191.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =      37.23 ms /    29 tokens (    1.28 ms per token,   778.84 tokens per second)\n",
      "llama_print_timings:        eval time =      85.83 ms /     9 runs   (    9.54 ms per token,   104.86 tokens per second)\n",
      "llama_print_timings:       total time =     201.46 ms /    38 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       9.53 ms /    22 runs   (    0.43 ms per token,  2309.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =      83.29 ms /    23 tokens (    3.62 ms per token,   276.14 tokens per second)\n",
      "llama_print_timings:        eval time =     197.23 ms /    21 runs   (    9.39 ms per token,   106.47 tokens per second)\n",
      "llama_print_timings:       total time =     663.22 ms /    44 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       8.50 ms /    21 runs   (    0.40 ms per token,  2469.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =      35.44 ms /    28 tokens (    1.27 ms per token,   790.09 tokens per second)\n",
      "llama_print_timings:        eval time =     187.71 ms /    20 runs   (    9.39 ms per token,   106.55 tokens per second)\n",
      "llama_print_timings:       total time =     351.26 ms /    48 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      35.63 ms /    23 runs   (    1.55 ms per token,   645.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =      27.64 ms /    18 tokens (    1.54 ms per token,   651.18 tokens per second)\n",
      "llama_print_timings:        eval time =     218.71 ms /    22 runs   (    9.94 ms per token,   100.59 tokens per second)\n",
      "llama_print_timings:       total time =    1302.05 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      12.92 ms /    30 runs   (    0.43 ms per token,  2322.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =      27.90 ms /    22 tokens (    1.27 ms per token,   788.62 tokens per second)\n",
      "llama_print_timings:        eval time =     274.19 ms /    29 runs   (    9.45 ms per token,   105.77 tokens per second)\n",
      "llama_print_timings:       total time =     562.67 ms /    51 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       8.62 ms /    19 runs   (    0.45 ms per token,  2203.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =      22.65 ms /    13 tokens (    1.74 ms per token,   573.85 tokens per second)\n",
      "llama_print_timings:        eval time =     172.49 ms /    18 runs   (    9.58 ms per token,   104.35 tokens per second)\n",
      "llama_print_timings:       total time =     340.26 ms /    31 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      15.80 ms /    39 runs   (    0.41 ms per token,  2467.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =      27.73 ms /    19 tokens (    1.46 ms per token,   685.08 tokens per second)\n",
      "llama_print_timings:        eval time =     358.86 ms /    38 runs   (    9.44 ms per token,   105.89 tokens per second)\n",
      "llama_print_timings:       total time =     757.16 ms /    57 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    11 runs   (    0.42 ms per token,  2401.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =      43.26 ms /    30 tokens (    1.44 ms per token,   693.43 tokens per second)\n",
      "llama_print_timings:        eval time =     100.99 ms /    10 runs   (   10.10 ms per token,    99.02 tokens per second)\n",
      "llama_print_timings:       total time =     337.63 ms /    40 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      77.15 ms /   185 runs   (    0.42 ms per token,  2397.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =      25.75 ms /    12 tokens (    2.15 ms per token,   466.11 tokens per second)\n",
      "llama_print_timings:        eval time =    1865.41 ms /   184 runs   (   10.14 ms per token,    98.64 tokens per second)\n",
      "llama_print_timings:       total time =    2235.08 ms /   196 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       9.14 ms /    22 runs   (    0.42 ms per token,  2406.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =      32.41 ms /    20 tokens (    1.62 ms per token,   617.07 tokens per second)\n",
      "llama_print_timings:        eval time =     206.08 ms /    21 runs   (    9.81 ms per token,   101.90 tokens per second)\n",
      "llama_print_timings:       total time =     814.28 ms /    41 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      20.47 ms /    53 runs   (    0.39 ms per token,  2589.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =      35.39 ms /    25 tokens (    1.42 ms per token,   706.49 tokens per second)\n",
      "llama_print_timings:        eval time =     485.72 ms /    52 runs   (    9.34 ms per token,   107.06 tokens per second)\n",
      "llama_print_timings:       total time =     838.70 ms /    77 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       9.15 ms /    24 runs   (    0.38 ms per token,  2622.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =      22.77 ms /    16 tokens (    1.42 ms per token,   702.74 tokens per second)\n",
      "llama_print_timings:        eval time =     245.32 ms /    23 runs   (   10.67 ms per token,    93.76 tokens per second)\n",
      "llama_print_timings:       total time =     752.54 ms /    39 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      11.85 ms /    29 runs   (    0.41 ms per token,  2447.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =      48.58 ms /    35 tokens (    1.39 ms per token,   720.42 tokens per second)\n",
      "llama_print_timings:        eval time =     271.37 ms /    28 runs   (    9.69 ms per token,   103.18 tokens per second)\n",
      "llama_print_timings:       total time =     549.14 ms /    63 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       5.43 ms /    13 runs   (    0.42 ms per token,  2393.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =      27.65 ms /    17 tokens (    1.63 ms per token,   614.89 tokens per second)\n",
      "llama_print_timings:        eval time =     119.26 ms /    12 runs   (    9.94 ms per token,   100.62 tokens per second)\n",
      "llama_print_timings:       total time =     674.84 ms /    29 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      15.21 ms /    38 runs   (    0.40 ms per token,  2498.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =      48.00 ms /    38 tokens (    1.26 ms per token,   791.62 tokens per second)\n",
      "llama_print_timings:        eval time =     350.85 ms /    37 runs   (    9.48 ms per token,   105.46 tokens per second)\n",
      "llama_print_timings:       total time =     426.92 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      15.02 ms /    38 runs   (    0.40 ms per token,  2529.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =      40.94 ms /    25 tokens (    1.64 ms per token,   610.69 tokens per second)\n",
      "llama_print_timings:        eval time =     355.28 ms /    37 runs   (    9.60 ms per token,   104.14 tokens per second)\n",
      "llama_print_timings:       total time =     671.56 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       6.93 ms /    18 runs   (    0.38 ms per token,  2598.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =      63.65 ms /    16 tokens (    3.98 ms per token,   251.39 tokens per second)\n",
      "llama_print_timings:        eval time =     160.25 ms /    17 runs   (    9.43 ms per token,   106.08 tokens per second)\n",
      "llama_print_timings:       total time =     751.11 ms /    33 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      13.41 ms /    34 runs   (    0.39 ms per token,  2535.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =      37.77 ms /    14 tokens (    2.70 ms per token,   370.67 tokens per second)\n",
      "llama_print_timings:        eval time =     306.48 ms /    33 runs   (    9.29 ms per token,   107.68 tokens per second)\n",
      "llama_print_timings:       total time =     730.23 ms /    47 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =     186.41 ms /   463 runs   (    0.40 ms per token,  2483.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =      35.25 ms /    25 tokens (    1.41 ms per token,   709.14 tokens per second)\n",
      "llama_print_timings:        eval time =    4700.56 ms /   462 runs   (   10.17 ms per token,    98.29 tokens per second)\n",
      "llama_print_timings:       total time =    6959.45 ms /   487 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       6.20 ms /    15 runs   (    0.41 ms per token,  2418.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =      32.48 ms /    20 tokens (    1.62 ms per token,   615.69 tokens per second)\n",
      "llama_print_timings:        eval time =     161.04 ms /    14 runs   (   11.50 ms per token,    86.93 tokens per second)\n",
      "llama_print_timings:       total time =    1071.55 ms /    34 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       7.47 ms /    17 runs   (    0.44 ms per token,  2274.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =      42.77 ms /    31 tokens (    1.38 ms per token,   724.77 tokens per second)\n",
      "llama_print_timings:        eval time =     162.77 ms /    16 runs   (   10.17 ms per token,    98.30 tokens per second)\n",
      "llama_print_timings:       total time =     391.11 ms /    47 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       2.68 ms /     6 runs   (    0.45 ms per token,  2239.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =      35.27 ms /    25 tokens (    1.41 ms per token,   708.80 tokens per second)\n",
      "llama_print_timings:        eval time =      57.30 ms /     5 runs   (   11.46 ms per token,    87.26 tokens per second)\n",
      "llama_print_timings:       total time =     407.04 ms /    30 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      67.81 ms /   169 runs   (    0.40 ms per token,  2492.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =      28.26 ms /    24 tokens (    1.18 ms per token,   849.20 tokens per second)\n",
      "llama_print_timings:        eval time =    1674.23 ms /   168 runs   (    9.97 ms per token,   100.34 tokens per second)\n",
      "llama_print_timings:       total time =    1966.24 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      13.00 ms /    31 runs   (    0.42 ms per token,  2385.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =      26.36 ms /    16 tokens (    1.65 ms per token,   606.93 tokens per second)\n",
      "llama_print_timings:        eval time =     335.92 ms /    30 runs   (   11.20 ms per token,    89.31 tokens per second)\n",
      "llama_print_timings:       total time =    1155.90 ms /    46 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       6.21 ms /    12 runs   (    0.52 ms per token,  1933.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =      23.08 ms /    16 tokens (    1.44 ms per token,   693.27 tokens per second)\n",
      "llama_print_timings:        eval time =     106.63 ms /    11 runs   (    9.69 ms per token,   103.16 tokens per second)\n",
      "llama_print_timings:       total time =     442.44 ms /    27 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      34.74 ms /    83 runs   (    0.42 ms per token,  2388.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =      35.96 ms /    31 tokens (    1.16 ms per token,   862.16 tokens per second)\n",
      "llama_print_timings:        eval time =     824.00 ms /    82 runs   (   10.05 ms per token,    99.51 tokens per second)\n",
      "llama_print_timings:       total time =    1319.09 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      21.94 ms /    52 runs   (    0.42 ms per token,  2370.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =      35.39 ms /    26 tokens (    1.36 ms per token,   734.63 tokens per second)\n",
      "llama_print_timings:        eval time =     487.24 ms /    51 runs   (    9.55 ms per token,   104.67 tokens per second)\n",
      "llama_print_timings:       total time =    1607.81 ms /    77 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       3.61 ms /     8 runs   (    0.45 ms per token,  2214.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =      35.80 ms /    29 tokens (    1.23 ms per token,   810.15 tokens per second)\n",
      "llama_print_timings:        eval time =      66.78 ms /     7 runs   (    9.54 ms per token,   104.82 tokens per second)\n",
      "llama_print_timings:       total time =     879.22 ms /    36 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       5.71 ms /    14 runs   (    0.41 ms per token,  2453.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =      26.50 ms /    16 tokens (    1.66 ms per token,   603.86 tokens per second)\n",
      "llama_print_timings:        eval time =     132.18 ms /    13 runs   (   10.17 ms per token,    98.35 tokens per second)\n",
      "llama_print_timings:       total time =     247.58 ms /    29 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      34.83 ms /    83 runs   (    0.42 ms per token,  2382.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     300.74 ms /    42 tokens (    7.16 ms per token,   139.65 tokens per second)\n",
      "llama_print_timings:        eval time =     823.91 ms /    82 runs   (   10.05 ms per token,    99.52 tokens per second)\n",
      "llama_print_timings:       total time =    2764.33 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =      51.18 ms /    40 runs   (    1.28 ms per token,   781.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =      28.23 ms /    23 tokens (    1.23 ms per token,   814.76 tokens per second)\n",
      "llama_print_timings:        eval time =     378.45 ms /    39 runs   (    9.70 ms per token,   103.05 tokens per second)\n",
      "llama_print_timings:       total time =    1081.39 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =       7.29 ms /    17 runs   (    0.43 ms per token,  2332.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =      35.42 ms /    27 tokens (    1.31 ms per token,   762.22 tokens per second)\n",
      "llama_print_timings:        eval time =     153.11 ms /    16 runs   (    9.57 ms per token,   104.50 tokens per second)\n",
      "llama_print_timings:       total time =     541.56 ms /    43 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     199.07 ms\n",
      "llama_print_timings:      sample time =     253.77 ms /   512 runs   (    0.50 ms per token,  2017.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =      22.75 ms /    13 tokens (    1.75 ms per token,   571.45 tokens per second)\n",
      "llama_print_timings:        eval time =    5236.73 ms /   511 runs   (   10.25 ms per token,    97.58 tokens per second)\n",
      "llama_print_timings:       total time =    8818.54 ms /   524 tokens\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "configs = [\n",
    "    ('k50_p0.90_t0.1', dict(top_k=50, top_p=0.90, temperature=0.1)),\n",
    "    ('k50_p0.95_t0.1', dict(top_k=50, top_p=0.95, temperature=0.1)),\n",
    "    ('k50_p0.90_t0.2', dict(top_k=50, top_p=0.90, temperature=0.2)),\n",
    "    ('k50_p0.95_t0.2', dict(top_k=50, top_p=0.95, temperature=0.2)),\n",
    "    ('k50_p0.90_t0.3', dict(top_k=50, top_p=0.90, temperature=0.3)),\n",
    "    ('k50_p0.95_t0.3', dict(top_k=50, top_p=0.95, temperature=0.3)),\n",
    "    ('k75_p0.90_t0.1', dict(top_k=75, top_p=0.90, temperature=0.1)),\n",
    "    ('k75_p0.95_t0.1', dict(top_k=75, top_p=0.95, temperature=0.1)),\n",
    "    ('k75_p0.90_t0.2', dict(top_k=75, top_p=0.90, temperature=0.2)),\n",
    "    ('k75_p0.95_t0.2', dict(top_k=75, top_p=0.95, temperature=0.2)),\n",
    "    ('k75_p0.90_t0.3', dict(top_k=75, top_p=0.90, temperature=0.3)),\n",
    "    ('k75_p0.95_t0.3', dict(top_k=75, top_p=0.95, temperature=0.3)),\n",
    "]\n",
    "\n",
    "random.shuffle(configs)\n",
    "\n",
    "for shorthand, config in tqdm.tqdm(configs, desc='configs'):\n",
    "    with open(f'mistral-answers-with-logprobs.{shorthand}.jsonl', 'w') as ostr_logprobs, \\\n",
    "    open(f'mistral-answers.{shorthand}.jsonl', 'w') as ostr:\n",
    "        for record in tqdm.tqdm(records, desc='items'):\n",
    "            message = record['question']\n",
    "            prompt = f\"[INST] {message} [/INST]\"\n",
    "            if 'alt_question' in record:\n",
    "                del record['alt_question']\n",
    "            response = lcpp_llm(\n",
    "                prompt=prompt,\n",
    "                logprobs=32_000,\n",
    "                max_tokens=512,\n",
    "                **config\n",
    "            )\n",
    "            print(\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        **record, \n",
    "                        'model_output': response['choices'][0]['text'],\n",
    "                        'tokens': response['choices'][0]['logprobs']['tokens'],\n",
    "                        'logprobs': [\n",
    "                            {k: float(v) for k,v in d.items()} \n",
    "                            for d in response['choices'][0]['logprobs']['top_logprobs']\n",
    "                        ],\n",
    "                        'lang': 'DE',\n",
    "                    }\n",
    "                ), \n",
    "                file=ostr_logprobs,\n",
    "            )\n",
    "            \n",
    "            print(\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        **record, \n",
    "                        'model_output': response['choices'][0]['text'],\n",
    "                        'tokens': response['choices'][0]['logprobs']['tokens'],\n",
    "                        'lang': 'EN',\n",
    "                    }\n",
    "                ), \n",
    "                file=ostr,\n",
    "                flush=True,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04b2b191f387469facbc7e0f63edd957": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c18583fabf94cf88d89e9d0ad83cd46",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_16ceb8ceabea4adeb2ed5d3c62a52e87",
      "value": 1
     }
    },
    "07bb3c8d23084467b680d0f8be879bcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08db236b9ee74ccb9ac456bf09e298e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ea36c0ff6cd4559bf733fb73ff82693": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fca89659d3684477bb46613bbb96383d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_265b13864e334d2d8875d1de157c428a",
      "value": 1
     }
    },
    "16ceb8ceabea4adeb2ed5d3c62a52e87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1c18583fabf94cf88d89e9d0ad83cd46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "265b13864e334d2d8875d1de157c428a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2abefc6082af406ab1c955a880a2b419": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ae89d1a8a074a249b750d138587e44d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eb30e73c1e824fa8942f0c58104d696f",
       "IPY_MODEL_df0a135d8a5b43d5ab94bef15b2db5aa",
       "IPY_MODEL_a5e99c0d3739407799fde2f29a301d05"
      ],
      "layout": "IPY_MODEL_fa5555299e2e47ae9d2cc7a7e58415f4"
     }
    },
    "2b25549d8eac4efd99bf1beb4fb26b0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e1566a3d2f64b5fbbaf7cc51b9c9902": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48be64dd9497468f83d73bd119591271": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cebd82bbc195424a908c9527ee1a21d3",
      "placeholder": "​",
      "style": "IPY_MODEL_8665cfefbc984fc4873e73cd96d6c018",
      "value": "Downloading data files: 100%"
     }
    },
    "4f891d2316604dd08cd5ffd22c8854d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c70248a7e6e45199ed626fa68037174",
      "placeholder": "​",
      "style": "IPY_MODEL_07bb3c8d23084467b680d0f8be879bcd",
      "value": "Generating val split: "
     }
    },
    "4facca9ecbd74aa5b4dc474634686064": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c70248a7e6e45199ed626fa68037174": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c4a2676871e492897d305d6d9a6fac9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "823cdbf0fa2c43559d01de4664258a86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8665cfefbc984fc4873e73cd96d6c018": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86da540e05824f2c95b5c8bea9b4581d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d1f94d67f08449439e3191bcdf87c6bf",
       "IPY_MODEL_cb886b4dac084c0290e1fd1c229b092e",
       "IPY_MODEL_8b8fd80c79c54e479b15f798bc545b96"
      ],
      "layout": "IPY_MODEL_3e1566a3d2f64b5fbbaf7cc51b9c9902"
     }
    },
    "8b8fd80c79c54e479b15f798bc545b96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08db236b9ee74ccb9ac456bf09e298e1",
      "placeholder": "​",
      "style": "IPY_MODEL_977e8b1928ec42a285804dcc8fc13cb5",
      "value": " 1/1 [00:00&lt;00:00,  1.86it/s]"
     }
    },
    "977e8b1928ec42a285804dcc8fc13cb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f4e1bc76cfb4643877686a6f0271b52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ceffacff7f492d87084da291061006": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0f2fe09ab0a4a21acda513f96bb7faf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4f891d2316604dd08cd5ffd22c8854d9",
       "IPY_MODEL_0ea36c0ff6cd4559bf733fb73ff82693",
       "IPY_MODEL_de38e0a8f5a24cbdbf755db3cfd399ec"
      ],
      "layout": "IPY_MODEL_9f4e1bc76cfb4643877686a6f0271b52"
     }
    },
    "a5e99c0d3739407799fde2f29a301d05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e35a5293e19748679095d1222f1a31e5",
      "placeholder": "​",
      "style": "IPY_MODEL_2abefc6082af406ab1c955a880a2b419",
      "value": " 5.94G/5.94G [00:45&lt;00:00, 157MB/s]"
     }
    },
    "ac217ebd99d94729ac89ed81fc0a0ab5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeaed97ed3f441e9aa2ce24c87e02d87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af87959da48a436e842f58ac691717df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aff193ecfc2e4d5a8b3ddd4f63604e63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48be64dd9497468f83d73bd119591271",
       "IPY_MODEL_04b2b191f387469facbc7e0f63edd957",
       "IPY_MODEL_e225b3758fa24df3a0d6f1a039d3220a"
      ],
      "layout": "IPY_MODEL_aeaed97ed3f441e9aa2ce24c87e02d87"
     }
    },
    "c96a1b051a7b4fbfbd873be07cf44cf0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb886b4dac084c0290e1fd1c229b092e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4facca9ecbd74aa5b4dc474634686064",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f52b2088b6724e6dad9ee18ba364c009",
      "value": 1
     }
    },
    "cebd82bbc195424a908c9527ee1a21d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1f94d67f08449439e3191bcdf87c6bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac217ebd99d94729ac89ed81fc0a0ab5",
      "placeholder": "​",
      "style": "IPY_MODEL_2b25549d8eac4efd99bf1beb4fb26b0c",
      "value": "Extracting data files: 100%"
     }
    },
    "de38e0a8f5a24cbdbf755db3cfd399ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_823cdbf0fa2c43559d01de4664258a86",
      "placeholder": "​",
      "style": "IPY_MODEL_e5ae38c7214c4f05974de99e5d5c3485",
      "value": " 499/0 [00:00&lt;00:00, 2393.49 examples/s]"
     }
    },
    "df0a135d8a5b43d5ab94bef15b2db5aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0ceffacff7f492d87084da291061006",
      "max": 5942065440,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af87959da48a436e842f58ac691717df",
      "value": 5942065440
     }
    },
    "e225b3758fa24df3a0d6f1a039d3220a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c4a2676871e492897d305d6d9a6fac9",
      "placeholder": "​",
      "style": "IPY_MODEL_f432e32a03704652a5bcd21c7ce36abd",
      "value": " 1/1 [00:00&lt;00:00, 29.62it/s]"
     }
    },
    "e35a5293e19748679095d1222f1a31e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5ae38c7214c4f05974de99e5d5c3485": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb30e73c1e824fa8942f0c58104d696f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c96a1b051a7b4fbfbd873be07cf44cf0",
      "placeholder": "​",
      "style": "IPY_MODEL_fa37a3f2205749468f31309b6061ffef",
      "value": "mistral-7b-instruct-v0.2.Q6_K.gguf: 100%"
     }
    },
    "f432e32a03704652a5bcd21c7ce36abd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f52b2088b6724e6dad9ee18ba364c009": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fa37a3f2205749468f31309b6061ffef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa5555299e2e47ae9d2cc7a7e58415f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fca89659d3684477bb46613bbb96383d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
